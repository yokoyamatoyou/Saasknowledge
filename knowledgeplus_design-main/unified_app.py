import streamlit as st
import logging
import uuid
from datetime import datetime

# Import shared modules
from shared.chat_controller import ChatController, get_persona_list
from shared.search_engine import HybridSearchEngine
from shared.file_processor import FileProcessor
from shared.upload_utils import ensure_openai_key, BASE_KNOWLEDGE_DIR
from ui_modules.theme import apply_intel_theme
from shared.chat_history_utils import (
    load_chat_histories,
    create_history,
    append_message,
    update_title,
)

# Import functions from knowledge_gpt_app.app (some might be moved later)
from knowledge_gpt_app.app import (
    list_knowledge_bases,
    semantic_chunking,
    get_openai_client,
    refresh_search_engine,
    read_file as app_read_file, # Rename to avoid conflict with FileProcessor
    search_multiple_knowledge_bases
)

# Import FAQ generation (assuming it's a standalone script)
from generate_faq import generate_faqs_from_chunks

from config import DEFAULT_KB_NAME

logger = logging.getLogger(__name__)

# Early check for OpenAI API key
try:
    ensure_openai_key()
except Exception as e:
    st.error(f"OpenAI API key error: {e}")
    st.stop()

# Global page config and styling
st.set_page_config(
    layout="wide", page_title="KNOWLEDGE+", initial_sidebar_state="collapsed"
)

apply_intel_theme(st)

st.markdown("""
<style>
/* General styling for Google-like simplicity */
html, body, [class*="st-"] {
    font-family: 'Google Sans', 'Roboto', Arial, sans-serif;
    color: #3C4043; /* Google Grey 800 */
    background-color: #FFFFFF; /* White background */
}

/* Main container to center content and limit width */
.main .block-container {
    max-width: 850px; /* Similar to Google search results width */
    padding-top: 2rem; 
    padding-bottom: 2rem;
    padding-left: 1rem;
    padding-right: 1rem;
}

/* Title styling - smaller and more subtle */
h1 {
    font-size: 2.2rem; /* Adjusted for smaller, more Google-like title */
    color: #202124; /* Google Grey 900 */
    text-align: center;
    margin-bottom: 2rem;
}

/* Search input styling - rounded, subtle border */
[data-testid="stTextInput"] input {
    border-color: #dfe1e5; /* Google Grey 300 */
    border-radius: 24px; /* Pill shape */
    padding: 10px 20px;
    box-shadow: none; /* Remove default Streamlit shadow */
    transition: box-shadow 0.3s ease-in-out, border-color 0.3s ease-in-out;
}
[data-testid="stTextInput"] input:hover {
    box-shadow: 0 1px 1px rgba(0,0,0,0.1); /* Subtle shadow on hover */
}
[data-testid="stTextInput"] input:focus {
    border-color: #1a73e8; /* Google Blue 500 */
    box-shadow: 0 1px 1px rgba(0,0,0,0.1), 0 0 0 1px #1a73e8; /* Blue glow on focus */
    outline: none; /* Remove default outline */
}

/* Primary Button styling - Google Blue */
[data-testid="stButton"] > button {
    background-color: #1a73e8; /* Google Blue 500 */
    color: #FFFFFF; /* White text */
    border-radius: 4px;
    border: none;
    padding: 10px 24px;
    font-weight: 500;
    transition: background-color 0.3s ease-in-out, box-shadow 0.3s ease-in-out;
}
[data-testid="stButton"] > button:hover {
    background-color: #1765cc; /* Darker blue on hover */
    box-shadow: 0 1px 2px rgba(0,0,0,0.15);
}
[data-testid="stButton"] > button:active {
    background-color: #145cb8; /* Even darker on active */
}

/* Secondary Button styling */
[data-testid="stButton"] > button.secondary {
    background-color: #f8f9fa; /* Google Grey 100 */
    color: #3C4043; /* Google Grey 800 */
    border: 1px solid #dadce0; /* Google Grey 300 */
}
[data-testid="stButton"] > button.secondary:hover {
    background-color: #f0f0f0;
    box-shadow: 0 1px 2px rgba(0,0,0,0.1);
}

/* Card styling for search results */
.doc-card {
    border: 1px solid #dfe1e5;
    border-radius: 8px;
    padding: 16px;
    margin-bottom: 16px;
    box-shadow: 0 1px 2px 0 rgba(60,64,67,.3); /* Subtle shadow */
    background-color: #FFFFFF;
}

/* Expander styling */
.streamlit-expanderHeader {
    background-color: #f8f9fa; /* Light grey header */
    border-radius: 8px;
    border: 1px solid #dadce0;
    padding: 10px 16px;
}

/* Chat input styling - improved visibility */
.st-chat-input {
    border: 1px solid #dadce0; /* Add a clear border */
    border-radius: 8px;
    padding: 8px;
    background-color: #f8f9fa; /* Slightly different background */
}
.st-chat-input input {
    border: none; /* Remove inner input border */
    box-shadow: none; /* Remove inner input shadow */
}

/* Chat message styling */
.stChatMessage {
    padding: 10px 15px;
    border-radius: 18px;
    margin-bottom: 10px;
}
.stChatMessage.user {
    background-color: #e6f4ea; /* Light green for user */
    border-bottom-right-radius: 2px;
}
.stChatMessage.assistant {
    background-color: #e8f0fe; /* Light blue for assistant */
    border-bottom-left-radius: 2px;
}

/* Maximize chat space */
[data-testid="stVerticalBlock"] > div:nth-child(2) {
    flex: 1; /* Allow chat history to take available vertical space */
    overflow-y: auto; /* Enable scrolling for chat history */
}
</style>
""", unsafe_allow_html=True)

if "sidebar_visible" not in st.session_state:
    st.session_state["sidebar_visible"] = False

toggle_label = ">>" if not st.session_state.sidebar_visible else "<<"
if st.button(toggle_label, key="toggle_sidebar", help="„Çµ„Ç§„Éâ„Éê„Éº„ÅÆË°®Á§∫ÂàáÊõø"):
    st.session_state.sidebar_visible = not st.session_state.sidebar_visible
    st.rerun()

st.markdown(
    f"""
    <style>
    [data-testid="stSidebar"] {{
        transition: margin-left 0.3s ease;
        margin-left: {'0' if st.session_state.sidebar_visible else '-18rem'};
    }}
    </style>
    """,
    unsafe_allow_html=True,
)

def safe_generate_gpt_response(user_input, conversation_history=None, persona="default", temperature=None, response_length=None, client=None):
    """Wrapper around ChatController.generate_gpt_response with error handling."""
    try:
        gen = st.session_state.chat_controller.generate_gpt_response(
            user_input,
            conversation_history=conversation_history,
            persona=persona,
            temperature=temperature,
            response_length=response_length,
            client=client,
        )
        return gen
    except Exception as e:
        st.error(f"GPTÂøúÁ≠îÁîüÊàê„Ç®„É©„Éº: {e}")
        logger.error("GPT response generation error", exc_info=True)
        return None


def render_document_card(doc: dict) -> None:
    """Display a single search result using the `doc-card` style."""
    meta = doc.get("metadata", {}) or {}
    display_meta = meta.get("display_metadata", {}) or {}
    title = meta.get("title") or display_meta.get("title") or meta.get("filename", "No title")
    snippet = doc.get("text", "")[:120].replace("\n", " ")
    similarity = doc.get("similarity")
    from html import escape
    body = f"<div class='doc-card'><strong>{escape(title)}</strong>"
    if similarity is not None:
        body += f"<div>Score: {similarity:.3f}</div>"
    body += f"<div>{escape(snippet)}...</div></div>"
    st.markdown(body, unsafe_allow_html=True)

# --- Session State Initialization ---
if "current_mode" not in st.session_state:
    st.session_state["current_mode"] = "Ê§úÁ¥¢" # Default to Search mode
if "search_executed" not in st.session_state:
    st.session_state["search_executed"] = False
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []
if "gpt_conversation_id" not in st.session_state:
    st.session_state["gpt_conversation_id"] = str(uuid.uuid4())
if "gpt_conversation_title" not in st.session_state:
    st.session_state["gpt_conversation_title"] = "Êñ∞„Åó„ÅÑ‰ºöË©±"
if "persona" not in st.session_state:
    st.session_state["persona"] = "default"
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "response_length" not in st.session_state:
    st.session_state["response_length"] = "ÊôÆÈÄö"
if "rag_enabled" not in st.session_state:
    st.session_state["rag_enabled"] = True # Default to RAG enabled
if "prompt_advice" not in st.session_state:
    st.session_state["prompt_advice"] = False
if "title_generated" not in st.session_state:
    st.session_state["title_generated"] = False
if "chat_histories" not in st.session_state:
    st.session_state["chat_histories"] = load_chat_histories()
if "current_chat_id" not in st.session_state:
    if st.session_state["chat_histories"]:
        hist = st.session_state["chat_histories"][0]
        st.session_state["current_chat_id"] = hist["id"]
        st.session_state["chat_history"] = hist["messages"]
        st.session_state["gpt_conversation_title"] = hist["title"]
    else:
        st.session_state["current_chat_id"] = create_history({})

# --- Sidebar Navigation ---
# Use a key to persist selection and add emoji icons
mode_options = {
    "Ê§úÁ¥¢": "Ê§úÁ¥¢",
    "ÁÆ°ÁêÜ": "ÁÆ°ÁêÜ",
    "„ÉÅ„É£„ÉÉ„Éà": "„ÉÅ„É£„ÉÉ„Éà",
}

selected_mode_display = st.sidebar.radio(
    "„É°„Éã„É•„Éº", 
    list(mode_options.values()), 
    index=list(mode_options.keys()).index(st.session_state["current_mode"]),
    key="sidebar_mode_radio",
    help="„Ç¢„Éó„É™„Ç±„Éº„Ç∑„Éß„É≥„ÅÆ„É¢„Éº„Éâ„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ"
)
st.session_state["current_mode"] = list(mode_options.keys())[list(mode_options.values()).index(selected_mode_display)]

# Chat related sidebar controls
st.sidebar.markdown("---")
if st.sidebar.button("Ôºã Êñ∞„Åó„ÅÑ„ÉÅ„É£„ÉÉ„Éà", key="new_chat_btn"):
    new_id = create_history({
        "persona": st.session_state.get("persona"),
        "temperature": st.session_state.get("temperature"),
        "prompt_advice": st.session_state.get("prompt_advice"),
    })
    st.session_state.current_chat_id = new_id
    st.session_state.chat_history = []
    st.session_state.gpt_conversation_title = "Êñ∞„Åó„ÅÑ‰ºöË©±"
    st.session_state.title_generated = False
    st.session_state.chat_histories.insert(0, {
        "id": new_id,
        "title": "Êñ∞„Åó„ÅÑ‰ºöË©±",
        "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "settings": {},
        "messages": [],
    })

if st.session_state.chat_histories:
    with st.sidebar.expander("ÈÅéÂéª„ÅÆ‰ºöË©±", expanded=False):
        for hist in st.session_state.chat_histories:
            if st.button(hist["title"], key=f"load_{hist['id']}"):
                st.session_state.current_chat_id = hist["id"]
                st.session_state.chat_history = hist["messages"]
                st.session_state.gpt_conversation_title = hist["title"]
                st.session_state.title_generated = True
                st.rerun()

with st.sidebar.expander("„ÉÅ„É£„ÉÉ„ÉàË®≠ÂÆö", expanded=False):
    personas = get_persona_list()
    persona_ids = [p["id"] for p in personas]
    persona_names = {p["id"]: p.get("name", p["id"]) for p in personas}
    current_id = st.session_state.get("persona", persona_ids[0])
    selected_id = st.selectbox(
        "AI„Éö„É´„ÇΩ„Éä", persona_ids,
        index=persona_ids.index(current_id),
        format_func=lambda x: persona_names.get(x, x),
    )
    st.session_state.persona = selected_id
    st.session_state.temperature = st.slider(
        "Ê∏©Â∫¶", 0.0, 1.0, float(st.session_state.get("temperature", 0.7)), 0.05
    )
    st.session_state.prompt_advice = st.checkbox(
        "„Ç¢„Éâ„Éê„Ç§„Çπ„ÇíÊúâÂäπÂåñ", value=st.session_state.get("prompt_advice", False)
    )

# --- Main Content Area based on Mode ---
st.title("KNOWLEDGE+") # Always show the main title

if st.session_state["current_mode"] == "Ê§úÁ¥¢":
    # Search mode specific UI
    query = st.text_input(
        "main_search_box",
        placeholder="üîç „Ç≠„Éº„ÉØ„Éº„Éâ„ÅßÊ§úÁ¥¢„ÄÅ„Åæ„Åü„ÅØAI„Å∏„ÅÆË≥™Âïè„ÇíÂÖ•Âäõ...",
        label_visibility="collapsed",
        help="„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„Åã„ÇâÊÉÖÂ†±„ÇíÊ§úÁ¥¢„Åó„Åæ„Åô„ÄÇ"
    )

    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        if st.button("Ê§úÁ¥¢", type="primary", help="ÂÖ•Âäõ„Åï„Çå„Åü„Ç≠„Éº„ÉØ„Éº„Éâ„Åß„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„ÇíÊ§úÁ¥¢„Åó„Åæ„Åô„ÄÇ"):
            st.session_state["search_executed"] = True
            kb_names = [kb["name"] for kb in list_knowledge_bases()]
            st.session_state["results"], _ = search_multiple_knowledge_bases(
                query, kb_names
            )
            st.session_state["last_query"] = query
    with col2:
        if st.button("„ÇØ„É™„Ç¢", help="Ê§úÁ¥¢„Éú„ÉÉ„ÇØ„Çπ„Å®ÁµêÊûú„Çí„ÇØ„É™„Ç¢„Åó„Åæ„Åô„ÄÇ"):
            st.session_state["search_executed"] = False
            st.session_state["results"] = []
            st.session_state["last_query"] = ""
            st.rerun()

    if st.session_state.get("search_executed"):
        st.markdown("\n---") # Separator
        tabs = st.tabs(["AI„Å´„Çà„ÇãË¶ÅÁ¥Ñ", "Èñ¢ÈÄ£„Éä„É¨„ÉÉ„Ç∏‰∏ÄË¶ß"])
        with tabs[0]:
            results = st.session_state.get("results", [])
            if results:
                client = get_openai_client()
                if client:
                    context = "\n".join(r.get("text", "") for r in results[:3])
                    prompt = (
                        f"Ê¨°„ÅÆÊÉÖÂ†±„Åã„ÇâË≥™Âïè„Äé{st.session_state.get('last_query','')}„Äè„Å∏„ÅÆ"
                        f"Ë¶ÅÁ¥ÑÂõûÁ≠î„ÇíÁîüÊàê„Åó„Å¶„Åè„Å†„Åï„ÅÑ:\n{context}"
                    )
                    # Streaming for AI Summary
                    st.write("AI„ÅåË¶ÅÁ¥Ñ„ÇíÁîüÊàê‰∏≠...")
                    summary_placeholder = st.empty()
                    full_summary = ""
                    gen = safe_generate_gpt_response(
                        prompt,
                        conversation_history=[],
                        persona="default",
                        temperature=0.3,
                        response_length="Á∞°ÊΩî",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_summary += chunk
                            summary_placeholder.markdown(full_summary + "‚ñå")
                    summary_placeholder.markdown(full_summary) # Final content without cursor
                else:
                    st.info("Ë¶ÅÁ¥ÑÁîüÊàê„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")
            else:
                st.info("Ê§úÁ¥¢ÁµêÊûú„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ")
        with tabs[1]:
            for doc in st.session_state.get("results", []):
                render_document_card(doc)




if st.session_state["current_mode"] == "ÁÆ°ÁêÜ":
    st.subheader("ÁÆ°ÁêÜ")
    tabs = st.tabs(["„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„ÇπÊßãÁØâ", "FAQËá™ÂãïÁîüÊàê"])

    with tabs[0]:
        st.divider()
        with st.expander("„Éä„É¨„ÉÉ„Ç∏„ÇíËøΩÂä†„Åô„Çã", expanded=True):
            process_mode = st.radio("Âá¶ÁêÜ„É¢„Éº„Éâ", ["ÂÄãÂà•Âá¶ÁêÜ", "„Åæ„Å®„ÇÅ„Å¶Âá¶ÁêÜ"], help="„Éï„Ç°„Ç§„É´„ÇíÂÄãÂà•„Å´Âá¶ÁêÜ„Åô„Çã„Åã„ÄÅ„Åæ„Å®„ÇÅ„Å¶Âá¶ÁêÜ„Åô„Çã„Åã„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ")
            index_mode = st.radio("„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÊõ¥Êñ∞", ["Ëá™Âãï(Âá¶ÁêÜÂæå)", "ÊâãÂãï"], help="„Éï„Ç°„Ç§„É´Âá¶ÁêÜÂæå„Å´Ê§úÁ¥¢„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíËá™Âãï„ÅßÊõ¥Êñ∞„Åô„Çã„Åã„ÄÅÊâãÂãï„ÅßÊõ¥Êñ∞„Åô„Çã„Åã„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ")

            files = st.file_uploader(
                "„Éï„Ç°„Ç§„É´„ÇíÈÅ∏Êäû",
                type=FileProcessor.SUPPORTED_IMAGE_TYPES + FileProcessor.SUPPORTED_DOCUMENT_TYPES + FileProcessor.SUPPORTED_CAD_TYPES,
                accept_multiple_files=process_mode == "„Åæ„Å®„ÇÅ„Å¶Âá¶ÁêÜ",
                help="„Çµ„Éù„Éº„Éà„Åï„Çå„Å¶„ÅÑ„ÇãÁîªÂÉè„ÄÅ„Éâ„Ç≠„É•„É°„É≥„Éà„ÄÅCAD„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Åæ„Åô„ÄÇ",
            )

            if files:
                if not isinstance(files, list):
                    files = [files]

                for file in files:
                    with st.spinner(f"„Éï„Ç°„Ç§„É´„ÇíËß£Êûê‰∏≠: {file.name}..."):
                        text = app_read_file(file)
                    with st.spinner(f"„Éô„ÇØ„Éà„É´Âåñ„Åó„Å¶„ÅÑ„Åæ„Åô: {file.name}..."):
                        if text:
                            client = get_openai_client()
                            if client:
                                semantic_chunking(
                                    text,
                                    15,
                                    "C",
                                    "auto",
                                    DEFAULT_KB_NAME,
                                    client,
                                    original_filename=file.name,
                                    original_bytes=file.getvalue(),
                                    refresh=index_mode == "Ëá™Âãï(Âá¶ÁêÜÂæå)" and process_mode == "ÂÄãÂà•Âá¶ÁêÜ",
                                )

                if process_mode == "„Åæ„Å®„ÇÅ„Å¶Âá¶ÁêÜ" and index_mode == "Ëá™Âãï(Âá¶ÁêÜÂæå)":
                    refresh_search_engine(DEFAULT_KB_NAME)

                st.toast("„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÂÆå‰∫Ü")

            if index_mode == "ÊâãÂãï":
                if st.button("Ê§úÁ¥¢„Ç§„É≥„Éá„ÉÉ„ÇØ„ÇπÊõ¥Êñ∞"):
                    with st.spinner("Ê§úÁ¥¢„Ç®„É≥„Ç∏„É≥Êõ¥Êñ∞‰∏≠..."):
                        refresh_search_engine(DEFAULT_KB_NAME)
                    st.toast("Ê§úÁ¥¢„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÇíÊõ¥Êñ∞„Åó„Åæ„Åó„Åü")

    with tabs[1]:
        kb_name = st.text_input("Knowledge base name", value=DEFAULT_KB_NAME, help="FAQ„ÇíÁîüÊàê„Åô„Çã„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„ÅÆÂêçÂâç„ÇíÂÖ•Âäõ„Åó„Åæ„Åô„ÄÇ")
        max_tokens = st.number_input("Max tokens per chunk", 100, 2000, 1000, 100, help="„ÉÅ„É£„É≥„ÇØ„ÅÇ„Åü„Çä„ÅÆÊúÄÂ§ß„Éà„Éº„ÇØ„É≥Êï∞„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ")
        pairs = st.number_input("Pairs per chunk", 1, 10, 3, 1, help="ÂêÑ„ÉÅ„É£„É≥„ÇØ„Åã„ÇâÁîüÊàê„Åô„ÇãQ&A„Éö„Ç¢„ÅÆÊï∞„ÇíË®≠ÂÆö„Åó„Åæ„Åô„ÄÇ")
        if st.button("‚óé FAQÁîüÊàê", key="generate_faqs_btn", type="primary", help="Ë®≠ÂÆö„Å´Âü∫„Å•„ÅÑ„Å¶FAQ„ÇíÁîüÊàê„Åó„ÄÅ„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„Å´‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ"):
            client = get_openai_client()
            if not client:
                st.error("OpenAI„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÅÆÂèñÂæó„Å´Â§±Êïó„Åó„Åæ„Åó„Åü„ÄÇ")
            else:
                with st.spinner("FAQ„ÇíÁîüÊàê‰∏≠..."):
                    count = generate_faqs_from_chunks(kb_name, int(max_tokens), int(pairs), client=client)
                    refresh_search_engine(kb_name)
                st.success(f"{count}‰ª∂„ÅÆFAQ„ÇíÁîüÊàê„Åó„Åæ„Åó„Åü„ÄÇ")

if st.session_state["current_mode"] == "„ÉÅ„É£„ÉÉ„Éà":
    st.subheader("„ÉÅ„É£„ÉÉ„Éà")  # Subheader for current mode

    use_kb = st.checkbox(
        "ÂÖ®„Å¶„ÅÆ„Éä„É¨„ÉÉ„Ç∏„Åã„ÇâÊ§úÁ¥¢„Åô„Çã",
        value=st.session_state.get("rag_enabled", True),
    )
    st.session_state["rag_enabled"] = use_kb

    # „ÉÅ„É£„ÉÉ„ÉàÂ±•Ê≠¥Ë°®Á§∫„Ç®„É™„Ç¢
    chat_container = st.container(height=None) # Maximize vertical space

    with chat_container:
        for msg in st.session_state["chat_history"]:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    user_msg = st.chat_input("„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÈÄÅ‰ø°")
    if user_msg:
        st.session_state["chat_history"].append({"role": "user", "content": user_msg})
        append_message(st.session_state.current_chat_id, "user", user_msg)

        if st.session_state.get("prompt_advice"):
            client = get_openai_client()
            if client:
                advice_gen = safe_generate_gpt_response(
                    f"‰ª•‰∏ã„ÅÆ„É¶„Éº„Ç∂„Éº„Éó„É≠„É≥„Éó„Éà„Çí„ÄÅ„Çà„ÇäÊòéÁ¢∫„ÅßÂäπÊûúÁöÑ„Å™„Éó„É≠„É≥„Éó„Éà„Å´„Åô„Çã„Åü„ÇÅ„ÅÆÊîπÂñÑÊ°à„Çí„ÄÅÁ∞°ÊΩî„Å™ÁÆáÊù°Êõ∏„Åç„ÅÆMarkdownÂΩ¢Âºè„ÅßÊèêÊ°à„Åó„Å¶„Åè„Å†„Åï„ÅÑ:\n\n---\n{user_msg}\n---",
                    conversation_history=[],
                    persona="default",
                    temperature=0.0,
                    response_length="Á∞°ÊΩî",
                    client=client,
                )
                advice_text = ""
                if advice_gen:
                    for chunk in advice_gen:
                        advice_text += chunk
                st.info(f"üí° „Éó„É≠„É≥„Éó„Éà„Ç¢„Éâ„Éê„Ç§„Çπ:\n{advice_text}")
        
        context = ""
        if use_kb:
            # „Éä„É¨„ÉÉ„Ç∏Ê§úÁ¥¢„ÅåÊúâÂäπ„Å™Â†¥Âêà„ÅÆ„Åø„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„ÇíË™≠„ÅøËæº„Åø„ÄÅÊ§úÁ¥¢„ÇíÂÆüË°å
            # ChatController„ÅÆ„Ç§„É≥„Çπ„Çø„É≥„ÇπÂåñ„Çí„Åì„Åì„ÅßË°å„ÅÜ„Åì„Å®„Åß„ÄÅRAGÁÑ°ÂäπÊôÇ„ÅØ‰∏çË¶Å„Å™ÂàùÊúüÂåñ„ÇíÈÅø„Åë„Çã
            if "chat_controller" not in st.session_state or not isinstance(st.session_state.chat_controller, ChatController):
                try:
                    engine = HybridSearchEngine(str(BASE_KNOWLEDGE_DIR / DEFAULT_KB_NAME))
                    st.session_state.chat_controller = ChatController(engine)
                except Exception as e:
                    st.error(f"„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„ÅÆÂàùÊúüÂåñ„Å´Â§±Êïó„Åó„Åæ„Åó„Åü: {e}")
                    st.session_state.chat_controller = None # ÂàùÊúüÂåñÂ§±ÊïóÊôÇ„ÅØNone„ÇíË®≠ÂÆö

            if st.session_state.chat_controller:
                results, _ = search_multiple_knowledge_bases(user_msg, [DEFAULT_KB_NAME])
                context = "\n".join(r.get("text", "") for r in results[:3])
                if not context:
                    st.info("„Éä„É¨„ÉÉ„Ç∏Ê§úÁ¥¢„ÅßÈñ¢ÈÄ£ÊÉÖÂ†±„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇAI„ÅÆ‰∏ÄËà¨ÁöÑ„Å™Áü•Ë≠ò„ÅßÂõûÁ≠î„Åó„Åæ„Åô„ÄÇ")
            else:
                st.warning("„Éä„É¨„ÉÉ„Ç∏Ê§úÁ¥¢„ÅåÁÑ°ÂäπÂåñ„Åï„Çå„Å¶„ÅÑ„Çã„Åã„ÄÅ„Éä„É¨„ÉÉ„Ç∏„Éô„Éº„Çπ„ÅÆÂàùÊúüÂåñ„Å´Â§±Êïó„Åó„Åü„Åü„ÇÅ„ÄÅÊ§úÁ¥¢„ÅØË°å„Çè„Çå„Åæ„Åõ„Çì„ÄÇ")

        client = get_openai_client()
        if client:
            prompt = (
                f"Ê¨°„ÅÆÊÉÖÂ†±„ÇíÂèÇËÄÉ„Å´„É¶„Éº„Ç∂„Éº„ÅÆË≥™Âïè„Å´Á≠î„Åà„Å¶„Åè„Å†„Åï„ÅÑ:\n{context}\n\nË≥™Âïè:{user_msg}"
                if use_kb and context
                else user_msg
            )
            chat_temp = 0.2 if use_kb else float(st.session_state.get("temperature", 0.7))
            chat_persona = st.session_state.get("persona", "default")
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                # ChatController„ÅåÂàùÊúüÂåñ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥ÂêàÔºàRAGÁÑ°ÂäπÊôÇ„Å™„Å©Ôºâ„ÅØ„ÄÅÁõ¥Êé•GPTÂøúÁ≠î„ÇíÁîüÊàê
                if "chat_controller" not in st.session_state or st.session_state.chat_controller is None:
                    # Fallback to direct GPT response without RAG
                    gen = safe_generate_gpt_response(
                        user_msg,
                        conversation_history=[
                            {"role": m["role"], "content": m["content"]}
                            for m in st.session_state["chat_history"][:-1]
                            if m["role"] in ("user", "assistant")
                        ],
                        persona=chat_persona,
                        temperature=chat_temp,
                        response_length="ÊôÆÈÄö",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_response += chunk
                            message_placeholder.markdown(full_response + "‚ñå")
                else:
                    gen = safe_generate_gpt_response(
                        prompt,
                        conversation_history=[
                            {"role": m["role"], "content": m["content"]}
                            for m in st.session_state["chat_history"][:-1]
                            if m["role"] in ("user", "assistant")
                        ],
                        persona=chat_persona,
                        temperature=chat_temp,
                        response_length="ÊôÆÈÄö",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_response += chunk
                            message_placeholder.markdown(full_response + "‚ñå")
                message_placeholder.markdown(full_response) # Final content without cursor
            answer = full_response
        else:
            answer = "OpenAI„ÇØ„É©„Ç§„Ç¢„É≥„Éà„ÇíÂàùÊúüÂåñ„Åß„Åç„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ"
        st.session_state["chat_history"].append({"role": "assistant", "content": answer})
        append_message(st.session_state.current_chat_id, "assistant", answer)
        
        # 4. ‰ºöË©±„Çø„Ç§„Éà„É´ÁîüÊàê/Êõ¥Êñ∞
        if (
            not st.session_state.get("title_generated")
            and len(st.session_state.chat_history) >= 2
            and client
        ):
            current_history_for_title_gen = [
                m for m in st.session_state.chat_history if m["role"] in ["user", "assistant"]
            ]
            if current_history_for_title_gen:
                try:
                    if "chat_controller" in st.session_state and st.session_state.chat_controller:
                        new_title_val = st.session_state.chat_controller.generate_conversation_title(
                            current_history_for_title_gen, client
                        )
                        st.session_state.gpt_conversation_title = new_title_val
                        update_title(st.session_state.current_chat_id, new_title_val)
                        for h in st.session_state.chat_histories:
                            if h["id"] == st.session_state.current_chat_id:
                                h["title"] = new_title_val
                                break
                        st.session_state.title_generated = True
                        logger.info(f"‰ºöË©±„Çø„Ç§„Éà„É´„ÇíÊõ¥Êñ∞: {new_title_val}")
                except Exception as e:
                    logger.error(f"‰ºöË©±„Çø„Ç§„Éà„É´ÁîüÊàê„Ç®„É©„Éº: {e}", exc_info=True)

        st.rerun()


