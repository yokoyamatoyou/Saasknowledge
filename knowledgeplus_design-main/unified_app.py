from pathlib import Path
import streamlit as st
import logging
import uuid
from datetime import datetime
import io
import base64
from PIL import Image
import json
import re
import numpy as np
import pandas as pd
import shutil
import tempfile

# Import shared modules
from shared.chat_controller import ChatController, get_persona_list, load_persona
from shared.search_engine import HybridSearchEngine, search_knowledge_base
from shared.file_processor import FileProcessor
from shared.kb_builder import KnowledgeBuilder
from shared.upload_utils import ensure_openai_key, BASE_KNOWLEDGE_DIR

# Import functions from knowledge_gpt_app.app (some might be moved later)
from knowledge_gpt_app.app import (
    list_knowledge_bases,
    semantic_chunking,
    get_openai_client,
    refresh_search_engine,
    read_file as app_read_file, # Rename to avoid conflict with FileProcessor
    search_multiple_knowledge_bases
)

# Import FAQ generation (assuming it's a standalone script)
from generate_faq import generate_faqs_from_chunks

from config import DEFAULT_KB_NAME, EMBEDDING_MODEL, EMBEDDING_DIMENSIONS

logger = logging.getLogger(__name__)

# Global page config and styling
st.set_page_config(
    layout="wide", page_title="KNOWLEDGE+", initial_sidebar_state="collapsed"
)

st.markdown("""
<style>
/* General styling for Google-like simplicity */
html, body, [class*="st-"] {
    font-family: 'Google Sans', 'Roboto', Arial, sans-serif;
    color: #3C4043; /* Google Grey 800 */
    background-color: #FFFFFF; /* White background */
}

/* Main container to center content and limit width */
.main .block-container {
    max-width: 850px; /* Similar to Google search results width */
    padding-top: 2rem; 
    padding-bottom: 2rem;
    padding-left: 1rem;
    padding-right: 1rem;
}

/* Title styling - smaller and more subtle */
h1 {
    font-size: 2.2rem; /* Adjusted for smaller, more Google-like title */
    color: #202124; /* Google Grey 900 */
    text-align: center;
    margin-bottom: 2rem;
}

/* Search input styling - rounded, subtle border */
[data-testid="stTextInput"] input {
    border-color: #dfe1e5; /* Google Grey 300 */
    border-radius: 24px; /* Pill shape */
    padding: 10px 20px;
    box-shadow: none; /* Remove default Streamlit shadow */
    transition: box-shadow 0.3s ease-in-out, border-color 0.3s ease-in-out;
}
[data-testid="stTextInput"] input:hover {
    box-shadow: 0 1px 1px rgba(0,0,0,0.1); /* Subtle shadow on hover */
}
[data-testid="stTextInput"] input:focus {
    border-color: #1a73e8; /* Google Blue 500 */
    box-shadow: 0 1px 1px rgba(0,0,0,0.1), 0 0 0 1px #1a73e8; /* Blue glow on focus */
    outline: none; /* Remove default outline */
}

/* Primary Button styling - Google Blue */
[data-testid="stButton"] > button {
    background-color: #1a73e8; /* Google Blue 500 */
    color: #FFFFFF; /* White text */
    border-radius: 4px;
    border: none;
    padding: 10px 24px;
    font-weight: 500;
    transition: background-color 0.3s ease-in-out, box-shadow 0.3s ease-in-out;
}
[data-testid="stButton"] > button:hover {
    background-color: #1765cc; /* Darker blue on hover */
    box-shadow: 0 1px 2px rgba(0,0,0,0.15);
}
[data-testid="stButton"] > button:active {
    background-color: #145cb8; /* Even darker on active */
}

/* Secondary Button styling */
[data-testid="stButton"] > button.secondary {
    background-color: #f8f9fa; /* Google Grey 100 */
    color: #3C4043; /* Google Grey 800 */
    border: 1px solid #dadce0; /* Google Grey 300 */
}
[data-testid="stButton"] > button.secondary:hover {
    background-color: #f0f0f0;
    box-shadow: 0 1px 2px rgba(0,0,0,0.1);
}

/* Card styling for search results */
.doc-card {
    border: 1px solid #dfe1e5;
    border-radius: 8px;
    padding: 16px;
    margin-bottom: 16px;
    box-shadow: 0 1px 2px 0 rgba(60,64,67,.3); /* Subtle shadow */
    background-color: #FFFFFF;
}

/* Expander styling */
.streamlit-expanderHeader {
    background-color: #f8f9fa; /* Light grey header */
    border-radius: 8px;
    border: 1px solid #dadce0;
    padding: 10px 16px;
}

/* Chat input styling - improved visibility */
.st-chat-input {
    border: 1px solid #dadce0; /* Add a clear border */
    border-radius: 8px;
    padding: 8px;
    background-color: #f8f9fa; /* Slightly different background */
}
.st-chat-input input {
    border: none; /* Remove inner input border */
    box-shadow: none; /* Remove inner input shadow */
}

/* Chat message styling */
.stChatMessage {
    padding: 10px 15px;
    border-radius: 18px;
    margin-bottom: 10px;
}
.stChatMessage.user {
    background-color: #e6f4ea; /* Light green for user */
    border-bottom-right-radius: 2px;
}
.stChatMessage.assistant {
    background-color: #e8f0fe; /* Light blue for assistant */
    border-bottom-left-radius: 2px;
}

/* Maximize chat space */
[data-testid="stVerticalBlock"] > div:nth-child(2) {
    flex: 1; /* Allow chat history to take available vertical space */
    overflow-y: auto; /* Enable scrolling for chat history */
}
</style>
""", unsafe_allow_html=True)

if "sidebar_visible" not in st.session_state:
    st.session_state["sidebar_visible"] = False

toggle_label = ">>" if not st.session_state.sidebar_visible else "<<"
if st.button(toggle_label, key="toggle_sidebar", help="ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¡¨ç¤ºåˆ‡æ›¿"):
    st.session_state.sidebar_visible = not st.session_state.sidebar_visible
    st.rerun()

st.markdown(
    f"""
    <style>
    [data-testid="stSidebar"] {{
        transition: margin-left 0.3s ease;
        margin-left: {'0' if st.session_state.sidebar_visible else '-18rem'};
    }}
    </style>
    """,
    unsafe_allow_html=True,
)

def safe_generate_gpt_response(user_input, conversation_history=None, persona="default", temperature=None, response_length=None, client=None):
    """Wrapper around ChatController.generate_gpt_response with error handling."""
    try:
        gen = st.session_state.chat_controller.generate_gpt_response(
            user_input,
            conversation_history=conversation_history,
            persona=persona,
            temperature=temperature,
            response_length=response_length,
            client=client,
        )
        return gen
    except Exception as e:
        st.error(f"GPTå¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
        logger.error("GPT response generation error", exc_info=True)
        return None


def render_document_card(doc: dict) -> None:
    """Display a single search result using the `doc-card` style."""
    meta = doc.get("metadata", {}) or {}
    display_meta = meta.get("display_metadata", {}) or {}
    title = meta.get("title") or display_meta.get("title") or meta.get("filename", "No title")
    snippet = doc.get("text", "")[:120].replace("\n", " ")
    similarity = doc.get("similarity")
    from html import escape
    body = f"<div class='doc-card'><strong>{escape(title)}</strong>"
    if similarity is not None:
        body += f"<div>Score: {similarity:.3f}</div>"
    body += f"<div>{escape(snippet)}...</div></div>"
    st.markdown(body, unsafe_allow_html=True)

# --- Session State Initialization ---
if "current_mode" not in st.session_state:
    st.session_state["current_mode"] = "æ¤œç´¢" # Default to Search mode
if "search_executed" not in st.session_state:
    st.session_state["search_executed"] = False
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = []
if "gpt_conversation_id" not in st.session_state:
    st.session_state["gpt_conversation_id"] = str(uuid.uuid4())
if "gpt_conversation_title" not in st.session_state:
    st.session_state["gpt_conversation_title"] = "æ–°ã—ã„ä¼šè©±"
if "persona" not in st.session_state:
    st.session_state["persona"] = "default"
if "temperature" not in st.session_state:
    st.session_state["temperature"] = 0.7
if "response_length" not in st.session_state:
    st.session_state["response_length"] = "æ™®é€š"
if "rag_enabled" not in st.session_state:
    st.session_state["rag_enabled"] = True # Default to RAG enabled

# --- Sidebar Navigation ---
# Use a key to persist selection and add emoji icons
mode_options = {
    "æ¤œç´¢": "æ¤œç´¢",
    "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
    "ãƒãƒ£ãƒƒãƒˆ": "ãƒãƒ£ãƒƒãƒˆ",
    "FAQ": "FAQ"
}

selected_mode_display = st.sidebar.radio(
    "ãƒ¡ãƒ‹ãƒ¥ãƒ¼", 
    list(mode_options.values()), 
    index=list(mode_options.keys()).index(st.session_state["current_mode"]),
    key="sidebar_mode_radio",
    help="ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ¢ãƒ¼ãƒ‰ã‚’é¸æŠã—ã¾ã™ã€‚"
)
st.session_state["current_mode"] = list(mode_options.keys())[list(mode_options.values()).index(selected_mode_display)]

# --- Main Content Area based on Mode ---
st.title("KNOWLEDGE+") # Always show the main title

if st.session_state["current_mode"] == "æ¤œç´¢":
    # Search mode specific UI
    query = st.text_input(
        "main_search_box",
        placeholder="ğŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢ã€ã¾ãŸã¯AIã¸ã®è³ªå•ã‚’å…¥åŠ›...",
        label_visibility="collapsed",
        help="ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"
    )

    col1, col2, col3 = st.columns([1, 1, 4])
    with col1:
        if st.button("æ¤œç´¢", type="primary", help="å…¥åŠ›ã•ã‚ŒãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’æ¤œç´¢ã—ã¾ã™ã€‚"):
            st.session_state["search_executed"] = True
            kb_names = [kb["name"] for kb in list_knowledge_bases()]
            st.session_state["results"], _ = search_multiple_knowledge_bases(
                query, kb_names
            )
            st.session_state["last_query"] = query
    with col2:
        if st.button("ã‚¯ãƒªã‚¢", help="æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã¨çµæœã‚’ã‚¯ãƒªã‚¢ã—ã¾ã™ã€‚"):
            st.session_state["search_executed"] = False
            st.session_state["results"] = []
            st.session_state["last_query"] = ""
            st.rerun()

    if st.session_state.get("search_executed"):
        st.markdown("\n---") # Separator
        tabs = st.tabs(["AIã«ã‚ˆã‚‹è¦ç´„", "é–¢é€£ãƒŠãƒ¬ãƒƒã‚¸ä¸€è¦§"])
        with tabs[0]:
            results = st.session_state.get("results", [])
            if results:
                client = get_openai_client()
                if client:
                    context = "\n".join(r.get("text", "") for r in results[:3])
                    prompt = (
                        f"æ¬¡ã®æƒ…å ±ã‹ã‚‰è³ªå•ã€{st.session_state.get('last_query','')}ã€ã¸ã®"
                        f"è¦ç´„å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„:\n{context}"
                    )
                    # Streaming for AI Summary
                    st.write("AIãŒè¦ç´„ã‚’ç”Ÿæˆä¸­...")
                    summary_placeholder = st.empty()
                    full_summary = ""
                    gen = safe_generate_gpt_response(
                        prompt,
                        conversation_history=[],
                        persona="default",
                        temperature=0.3,
                        response_length="ç°¡æ½”",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_summary += chunk
                            summary_placeholder.markdown(full_summary + "â–Œ")
                    summary_placeholder.markdown(full_summary) # Final content without cursor
                else:
                    st.info("è¦ç´„ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
            else:
                st.info("æ¤œç´¢çµæœãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
        with tabs[1]:
            for doc in st.session_state.get("results", []):
                render_document_card(doc)


if st.session_state["current_mode"] == "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰":
    st.subheader("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰") # Subheader for current mode
    st.divider()
    with st.expander("ãƒŠãƒ¬ãƒƒã‚¸ã‚’è¿½åŠ ã™ã‚‹", expanded=True):
        process_mode = st.radio("å‡¦ç†ãƒ¢ãƒ¼ãƒ‰", ["å€‹åˆ¥å‡¦ç†", "ã¾ã¨ã‚ã¦å‡¦ç†"], help="ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«å‡¦ç†ã™ã‚‹ã‹ã€ã¾ã¨ã‚ã¦å‡¦ç†ã™ã‚‹ã‹ã‚’é¸æŠã—ã¾ã™ã€‚")
        index_mode = st.radio("ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°", ["è‡ªå‹•(å‡¦ç†å¾Œ)", "æ‰‹å‹•"], help="ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†å¾Œã«æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è‡ªå‹•ã§æ›´æ–°ã™ã‚‹ã‹ã€æ‰‹å‹•ã§æ›´æ–°ã™ã‚‹ã‹ã‚’é¸æŠã—ã¾ã™ã€‚")

        files = st.file_uploader(
            "ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ",
            type=FileProcessor.SUPPORTED_IMAGE_TYPES + FileProcessor.SUPPORTED_DOCUMENT_TYPES + FileProcessor.SUPPORTED_CAD_TYPES,
            accept_multiple_files=process_mode == "ã¾ã¨ã‚ã¦å‡¦ç†",
            help="ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ç”»åƒã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€CADãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
        )

        if files:
            if not isinstance(files, list):
                files = [files]

            for file in files:
                with st.spinner(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æä¸­: {file.name}..."):
                    text = app_read_file(file)
                with st.spinner(f"ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦ã„ã¾ã™: {file.name}..."):
                    if text:
                        client = get_openai_client()
                        if client:
                            semantic_chunking(
                                text,
                                15,
                                "C",
                                "auto",
                                DEFAULT_KB_NAME,
                                client,
                                original_filename=file.name,
                                original_bytes=file.getvalue(),
                                refresh=index_mode == "è‡ªå‹•(å‡¦ç†å¾Œ)" and process_mode == "å€‹åˆ¥å‡¦ç†",
                            )

            if process_mode == "ã¾ã¨ã‚ã¦å‡¦ç†" and index_mode == "è‡ªå‹•(å‡¦ç†å¾Œ)":
                refresh_search_engine(DEFAULT_KB_NAME)

            st.toast("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†")

        if index_mode == "æ‰‹å‹•":
            if st.button("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°", help="æ‰‹å‹•ã§æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã¾ã™ã€‚æ–°ã—ã„ãƒŠãƒ¬ãƒƒã‚¸ãŒæ¤œç´¢å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚"):
                with st.spinner("æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³æ›´æ–°ä¸­..."):
                    refresh_search_engine(DEFAULT_KB_NAME)
                st.toast("æ¤œç´¢ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°ã—ã¾ã—ãŸ")


if st.session_state["current_mode"] == "ãƒãƒ£ãƒƒãƒˆ":
    st.subheader("ãƒãƒ£ãƒƒãƒˆ") # Subheader for current mode
    
    # RAGæ¤œç´¢ã®æœ‰åŠ¹/ç„¡åŠ¹ãƒˆã‚°ãƒ«
    rag_enabled = st.sidebar.checkbox("RAGæ¤œç´¢ã‚’æœ‰åŠ¹ã«ã™ã‚‹", value=st.session_state["rag_enabled"], help="ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’å–å¾—ã—ã¦å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ç„¡åŠ¹ã«ã™ã‚‹ã¨ä¸€èˆ¬çš„ãªAIãƒãƒ£ãƒƒãƒˆã«ãªã‚Šã¾ã™ã€‚")
    st.session_state["rag_enabled"] = rag_enabled

    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´è¡¨ç¤ºã‚¨ãƒªã‚¢
    chat_container = st.container(height=None) # Maximize vertical space

    with chat_container:
        for msg in st.session_state["chat_history"]:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

    user_msg = st.chat_input("ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡")
    if user_msg:
        st.session_state["chat_history"].append({"role": "user", "content": user_msg})
        
        context = ""
        if rag_enabled:
            # RAGæ¤œç´¢ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã‚’èª­ã¿è¾¼ã¿ã€æ¤œç´¢ã‚’å®Ÿè¡Œ
            # ChatControllerã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã‚’ã“ã“ã§è¡Œã†ã“ã¨ã§ã€RAGç„¡åŠ¹æ™‚ã¯ä¸è¦ãªåˆæœŸåŒ–ã‚’é¿ã‘ã‚‹
            if "chat_controller" not in st.session_state or not isinstance(st.session_state.chat_controller, ChatController):
                try:
                    engine = HybridSearchEngine(str(BASE_KNOWLEDGE_DIR / DEFAULT_KB_NAME))
                    st.session_state.chat_controller = ChatController(engine)
                except Exception as e:
                    st.error(f"ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                    st.session_state.chat_controller = None # åˆæœŸåŒ–å¤±æ•—æ™‚ã¯Noneã‚’è¨­å®š

            if st.session_state.chat_controller:
                results, _ = search_multiple_knowledge_bases(user_msg, [DEFAULT_KB_NAME])
                context = "\n".join(r.get("text", "") for r in results[:3])
                if not context:
                    st.info("RAGæ¤œç´¢ã§é–¢é€£æƒ…å ±ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚AIã®ä¸€èˆ¬çš„ãªçŸ¥è­˜ã§å›ç­”ã—ã¾ã™ã€‚")
            else:
                st.warning("RAGæ¤œç´¢ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸãŸã‚ã€RAGæ¤œç´¢ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚")

        client = get_openai_client()
        if client:
            prompt = f"æ¬¡ã®æƒ…å ±ã‚’å‚è€ƒã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç­”ãˆã¦ãã ã•ã„:\n{context}\n\nè³ªå•:{user_msg}" if rag_enabled and context else user_msg
            
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                full_response = ""
                # ChatControllerãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ãªã„å ´åˆï¼ˆRAGç„¡åŠ¹æ™‚ãªã©ï¼‰ã¯ã€ç›´æ¥GPTå¿œç­”ã‚’ç”Ÿæˆ
                if "chat_controller" not in st.session_state or st.session_state.chat_controller is None:
                    # Fallback to direct GPT response without RAG
                    gen = safe_generate_gpt_response(
                        user_msg,
                        conversation_history=[
                            {"role": m["role"], "content": m["content"]}
                            for m in st.session_state["chat_history"][:-1]
                            if m["role"] in ("user", "assistant")
                        ],
                        persona="default",
                        temperature=0.3,
                        response_length="æ™®é€š",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_response += chunk
                            message_placeholder.markdown(full_response + "â–Œ")
                else:
                    gen = safe_generate_gpt_response(
                        prompt,
                        conversation_history=[
                            {"role": m["role"], "content": m["content"]}
                            for m in st.session_state["chat_history"][:-1]
                            if m["role"] in ("user", "assistant")
                        ],
                        persona="default",
                        temperature=0.3,
                        response_length="æ™®é€š",
                        client=client,
                    )
                    if gen:
                        for chunk in gen:
                            full_response += chunk
                            message_placeholder.markdown(full_response + "â–Œ")
                message_placeholder.markdown(full_response) # Final content without cursor
            answer = full_response
        else:
            answer = "OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        st.session_state["chat_history"].append({"role": "assistant", "content": answer})
        
        # 4. ä¼šè©±ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆ/æ›´æ–°
        if len(st.session_state.chat_history) >= 4 and client:
            current_history_for_title_gen = [m for m in st.session_state.chat_history if m["role"] in ["user", "assistant"]]
            if current_history_for_title_gen:
                try:
                    # generate_conversation_titleã¯ChatControllerã®ãƒ¡ã‚½ãƒƒãƒ‰
                    if "chat_controller" in st.session_state and st.session_state.chat_controller:
                        new_title_val = st.session_state.chat_controller.generate_conversation_title(current_history_for_title_gen, client)
                        if new_title_val != st.session_state.get('gpt_conversation_title'):
                            st.session_state.gpt_conversation_title = new_title_val
                            logger.info(f"ä¼šè©±ã‚¿ã‚¤ãƒˆãƒ«ã‚’æ›´æ–°: {new_title_val}")
                except Exception as e:
                    logger.error(f"ä¼šè©±ã‚¿ã‚¤ãƒˆãƒ«ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)

        st.rerun()


if st.session_state["current_mode"] == "FAQ":
    st.subheader("FAQ") # Subheader for current mode
    kb_name = st.text_input("Knowledge base name", value=DEFAULT_KB_NAME, help="FAQã‚’ç”Ÿæˆã™ã‚‹ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã®åå‰ã‚’å…¥åŠ›ã—ã¾ã™ã€‚")
    max_tokens = st.number_input("Max tokens per chunk", 100, 2000, 1000, 100, help="ãƒãƒ£ãƒ³ã‚¯ã‚ãŸã‚Šã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚")
    pairs = st.number_input("Pairs per chunk", 1, 10, 3, 1, help="å„ãƒãƒ£ãƒ³ã‚¯ã‹ã‚‰ç”Ÿæˆã™ã‚‹Q&Aãƒšã‚¢ã®æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚")
    if st.button("â— FAQã‚’ç”Ÿæˆ", key="generate_faqs_btn", type="primary", help="è¨­å®šã«åŸºã¥ã„ã¦FAQã‚’ç”Ÿæˆã—ã€ãƒŠãƒ¬ãƒƒã‚¸ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚"):
        client = get_openai_client()
        if not client:
            st.error("OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
        else:
            with st.spinner("FAQã‚’ç”Ÿæˆä¸­..."):
                count = generate_faqs_from_chunks(kb_name, int(max_tokens), int(pairs), client=client)
                refresh_search_engine(kb_name)
            st.success(f"{count}ä»¶ã®FAQã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
