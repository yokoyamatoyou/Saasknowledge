### **1\. 最終目標 (Overall Goal)**

既存のナレッジベース検索アプリケーションに、過去に作成した画像検索機能 (core/mm\_builder\_utils.py) を統合します。  
最終的に、以下の機能を持つマルチモーダル検索システムを構築します。

* ユーザーはテキストファイル（.txt, .pdf, .mdなど）と画像ファイル（.png, .jpgなど）の両方をアップロードできる。  
* アップロードされたテキストと画像は、\*\*共通のベクトル空間にベクトル化（Embedding）\*\*されて保存される。  
* ユーザーがテキストで検索すると、関連する**テキストの断片（チャンク）と画像**の両方が検索結果として表示される。  
* 現在発生している ValueError: too many values to unpack (expected 2\) のエラーを修正する。

### **2\. プロジェクトの主要ファイルと役割**

コーディングにあたり、以下のファイルの役割を理解してください。

* unified\_app.py: Streamlitアプリケーションのメインファイル。  
* ui\_modules/management\_ui.py: ファイルアップロードなど、ナレッジベース管理UIの描画を担当。**現在 ValueError が発生しているファイル。**  
* shared/file\_processor.py: アップロードされたファイルを種類別に処理する。テキスト抽出や画像のサムネイル生成を行う。**画像ベクトル化処理の追加が必要。**  
* shared/kb\_builder.py: テキストをチャンクに分割し、ベクトル化してナレッジベース（チャンクとベクトル）を構築する。**画像ベクトルも扱えるように拡張が必要。**  
* shared/search\_engine.py: テキストのキーワード検索（BM25）とベクトル検索を組み合わせたハイブリッド検索を実行する。**画像も検索対象に含める拡張が必要。**  
* core/mm\_builder\_utils.py: **（今回統合するファイル）** CLIPモデルを使い、画像とテキストをベクトル化する機能を持つ。現在はどこからも呼び出されていない。  
* config.py: アプリケーション全体の設定を管理する。モデル名やベクトル次元数などの設定を追加する。

### **3\. 実装の前提条件**

* **使用モデル:** テキストと画像のベクトル化には、両方を同じベクトル空間で扱えるマルチモーダルモデル CLIP を使用します。モデル名は **openai/clip-vit-base-patch32** を使用してください。  
* **ベクトル次元数:** 上記モデルの次元数は **512** です。この値をアプリケーション全体で統一的に使用します。  
* **必要なライブラリ:** requirements.txt に以下のライブラリが含まれていることを確認し、なければ追加してください。  
  * transformers  
  * torch  
  * faiss-cpu (または faiss-gpu)  
  * Pillow

### **4\. 実装の具体的なステップ**

以下のステップに従って、既存のコードを修正・拡張してください。

#### **ステップ1: ValueError の修正 (ui\_modules/management\_ui.py)**

まず、現在発生しているエラーを修正します。

* **問題:** ui\_modules/management\_ui.py の render\_management\_mode 関数内で、file\_processor.process\_file の戻り値を2つの変数 (image\_b64, cad\_meta) で受け取ろうとしていますが、この関数は辞書を1つ返すため、エラーが発生しています。  
* **指示:**  
  1. file\_processor.process\_file の戻り値を、まず1つの辞書型変数（例: processed\_data）で受け取るように修正してください。  
  2. その辞書から .get() メソッドを使って image\_base64 と metadata の値を安全に取り出し、後続の処理で使用してください。

#### **ステップ2: モデル設定の共通化 (config.py)**

モデル名と次元数を設定ファイルで一元管理できるようにします。

* **指示:** config.py に以下の設定変数を追加してください。アプリケーション全体でこれらの変数を参照するようにします。  
  \# config.py  
  \# ... 既存の設定 ...

  \# Multimodal Model Settings  
  MULTIMODAL\_MODEL \= "openai/clip-vit-base-patch32"  
  EMBEDDING\_DIM \= 512

#### **ステップ3: ナレッジベース構築プロセスの統合 (shared/kb\_builder.py と shared/file\_processor.py)**

テキストと画像の両方をベクトル化して保存できるように、ナレッジベースの構築プロセスを拡張します。

* **指示 (shared/kb\_builder.py の修正):**  
  1. KBBuilder クラスの \_\_init\_\_ で、core.mm\_builder\_utils から load\_model\_and\_processor を呼び出し、CLIPモデルとプロセッサを一度だけロードしてクラス変数として保持するようにしてください。（モデル名は config.MULTIMODAL\_MODEL を参照）  
  2. 既存の generate\_embeddings メソッドを修正します。このメソッドはOpenAIのAPIを直接呼び出す代わりに、\_\_init\_\_ でロードしたCLIPモデルを使ってテキストチャンクをベクトル化するように変更してください。  
  3. KBBuilder クラスに、画像ファイルをベクトル化するための新しいメソッド generate\_image\_embedding(self, image\_path) を追加してください。このメソッドは内部で core.mm\_builder\_utils.get\_image\_embedding を呼び出します。  
* **指示 (shared/file\_processor.py の修正):**  
  1. process\_file 関数を修正します。  
  2. ファイルが**画像**の場合、Pillowで画像を開いた後、KBBuilder のインスタンスを使って generate\_image\_embedding を呼び出し、画像のベクトルを取得してください。  
  3. 取得した画像ベクトルを、チャンクやメタデータと同様に、ナレッジベースの embeddings ディレクトリに保存する処理を追加してください。  
  4. 画像の場合、テキストチャンクの代わりとなる情報を定義します。例えば、ファイル名を本文とするような単純なチャンクデータを作成し、chunks ディレクトリに保存してください。これにより、検索結果で画像ファイル名を特定できます。

#### **ステップ4: 検索エンジンのマルチモーダル対応 (shared/search\_engine.py)**

検索エンジンが、テキストと画像の両方を含む統一ベクトル空間を検索できるように修正します。

* **指示 (shared/search\_engine.py の修正):**  
  1. HybridSearchEngine クラスの \_\_init\_\_ でも、KBBuilder と同様にCLIPモデルとプロセッサをロードするようにしてください。  
  2. search メソッドを修正します。ユーザーからの検索クエリ（テキスト）をベクトル化する際、OpenAI APIではなく、ロードしたCLIPモデルのテキストエンコーダーを使用してください。  
  3. ベクトル検索部分は、テキストと画像のベクトルが混在する embeddings ストアをそのまま検索対象とします。これにより、テキストクエリに意味的に近い画像もヒットするようになります。  
  4. BM25（キーワード検索）はテキストチャンクに対してのみ有効です。検索結果をマージする際、結果が画像チャンクだった場合はBM25スコアを0として扱うなど、適切に処理してください。  
  5. 検索結果を返す際、それがテキストチャンクなのか画像チャンクなのかを判別できるような情報（例: content\_type: 'text' or 'image'）を付与してください。

### **5\. 期待される最終的な動作**

上記の実装が完了すると、アプリケーションは以下のように動作するはずです。

1. ユーザーが管理画面からテキストファイルや画像ファイルをアップロードする。  
2. バックグラウンドで KBBuilder が動作し、テキストはチャンクに分割されCLIPでベクトル化、画像はそのままCLIPでベクトル化される。チャンクとベクトルがナレッジベースに保存される。  
3. ユーザーがチャット画面で質問（テキスト）を入力する。  
4. HybridSearchEngine が質問文をCLIPでベクトル化し、ナレッジベース内の全ベクトル（テキスト由来も画像由来も）と類似度を比較する。  
5. 類似度の高いものとして、関連するテキストの断片と**画像**の両方が検索結果として返され、UIに表示される。

以上の指示に基づき、リポジトリ全体のコードを修正してください。